{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/minhthonglai/ielts-gptj-tuner?scriptVersionId=242199688\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# GPT-J Fine-tuning for Essay Scoring","metadata":{}},{"cell_type":"markdown","source":"## System configurations ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when create a version using \"Save & Run All\" \n# Can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport sys\nimport os\n\n# Check Python version\nprint(f\"Python Version: `{sys.version}`\")  # Detailed version info\nprint(f\"Base Python location: `{sys.base_prefix}`\")\nprint(f\"Current Environment location: `{os.path.basename(sys.prefix)}`\", end='\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:15:35.121827Z","iopub.execute_input":"2025-05-27T16:15:35.122313Z","iopub.status.idle":"2025-05-27T16:15:35.129445Z","shell.execute_reply.started":"2025-05-27T16:15:35.12228Z","shell.execute_reply":"2025-05-27T16:15:35.128489Z"}},"outputs":[{"name":"stdout","text":"Python Version: `3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]`\nBase Python location: `/usr`\nCurrent Environment location: `usr`\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport time  # for CPU timing\n\n# Check if GPU is available and being used\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name()}\")\n    print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n\n# This code will work in both CPU and GPU environments\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n    \n# Test with a simple operation - modified to work on both CPU and GPU\nx = torch.rand(10000, 10000, device=device)\ny = torch.rand(10000, 10000, device=device)\n\nif device.type == \"cuda\":\n    # GPU timing approach\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    z = x @ y  # Matrix multiplication\n    end.record()\n    torch.cuda.synchronize()\n    print(f\"Operation time: {start.elapsed_time(end)} ms\")\nelse:\n    # CPU timing approach\n    start_time = time.time()\n    z = x @ y  # Matrix multiplication\n    end_time = time.time()\n    print(f\"Operation time: {(end_time - start_time) * 1000} ms\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install required libraries","metadata":{}},{"cell_type":"code","source":"# %pip install transformers peft datasets accelerate bitsandbytes trl\n# %pip install sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.484584Z","iopub.status.idle":"2025-05-27T16:17:47.484896Z","shell.execute_reply.started":"2025-05-27T16:17:47.484763Z","shell.execute_reply":"2025-05-27T16:17:47.484776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Libraries and Set Configuration","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport os\nfrom datasets import load_dataset\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU is available. Setting up GPU environment...\")\n    # Set maximum GPU memory usage to avoid OOM errors\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    torch.cuda.empty_cache()\nelse:\n    print(\"No GPU detected. Running in CPU-only mode.\")\n    # Ensure CUDA is disabled\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n# Enable memory efficient attention - works for both CPU and GPU\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # Work offline if model is downloaded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:09.883451Z","iopub.execute_input":"2025-05-27T16:17:09.883767Z","iopub.status.idle":"2025-05-27T16:17:40.925813Z","shell.execute_reply.started":"2025-05-27T16:17:09.883736Z","shell.execute_reply":"2025-05-27T16:17:40.924786Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 16:17:22.433098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748362642.680911      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748362642.753193      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"No GPU detected. Running in CPU-only mode.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Prepare the IELTS Dataset","metadata":{}},{"cell_type":"markdown","source":"### Sample code to create a dataset for IELTS assessment\n","metadata":{}},{"cell_type":"markdown","source":"Need to prepare the dataset with this structure:\n\n```\nFormat should be:\n{\n  \"input\": \"Rate this IELTS essay: [student essay text]\",\n  \"output\": \"Task Achievement: 7.0 - [detailed feedback]\\nCoherence and Cohesion: 6.5 - [detailed feedback]\\nLexical Resource: 6.0 - [detailed feedback]\\nGrammatical Range and Accuracy: 7.0 - [detailed feedback]\\nOverall Band Score: 6.5\"\n}\n```","metadata":{}},{"cell_type":"code","source":"# Example loading from CSV or creating from scratch\nfrom datasets import Dataset\n\n# Example data\ndata = {\n    \"input\": [\n        \"Rate this IELTS essay: In some countries, the number of shootings increase because many people have guns at home. To what extent do you agree or disagree? Some people believe that one of the causes of increasing crime is easy access to guns. In my opinion, I completely agree that shootings occur because of the availability of guns at home. [essay continues...]\",\n        # Add more examples\n    ],\n    \"output\": [\n        \"Task Achievement: 7.0 - The essay addresses all parts of the task and presents a clear position...\\nCoherence and Cohesion: 6.5 - The essay is generally well-organized...\\nLexical Resource: 6.0 - The essay uses an adequate range of vocabulary...\\nGrammatical Range and Accuracy: 7.0 - The essay demonstrates a good range of complex structures...\\nOverall Band Score: 6.5\",\n        # Add more examples\n    ]\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:40.926691Z","iopub.execute_input":"2025-05-27T16:17:40.927248Z","iopub.status.idle":"2025-05-27T16:17:40.933766Z","shell.execute_reply.started":"2025-05-27T16:17:40.927222Z","shell.execute_reply":"2025-05-27T16:17:40.931515Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset = Dataset.from_dict(data)\ntrain_dataset = dataset.shuffle().select(range(int(len(dataset)*0.9)))  # 90% for training\neval_dataset = dataset.shuffle().select(range(int(len(dataset)*0.9), len(dataset)))  # 10% for evaluation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:40.935277Z","iopub.execute_input":"2025-05-27T16:17:40.935937Z","iopub.status.idle":"2025-05-27T16:17:40.987832Z","shell.execute_reply.started":"2025-05-27T16:17:40.935882Z","shell.execute_reply":"2025-05-27T16:17:40.986172Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Load GPT-J with Quantization for Memory Efficiency","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"EleutherAI/gpt-j-6B\"\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU detected! Using 4-bit quantization...\")\n    # Define quantization configuration\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    \n    # Load with quantization for GPU\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=quantization_config,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\nelse:\n    print(\"No GPU detected! For development, using a smaller model instead...\")\n    # For CPU development, consider using a much smaller model\n    # GPT-J is too large for most CPU environments\n    cpu_model_id = \"distilgpt2\"  # Only 82M parameters vs 6B\n    model = AutoModelForCausalLM.from_pretrained(\n        cpu_model_id,\n        low_cpu_mem_usage=True\n    )\n    print(f\"⚠️ NOTE: Using {cpu_model_id} instead of {model_id} for CPU development!\")\n    print(\"⚠️ Remember to switch to GPU when ready for actual fine-tuning\")\n\n# Get the tokenizer for whatever model we loaded\ntokenizer = AutoTokenizer.from_pretrained(model_id if torch.cuda.is_available() else cpu_model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Set padding token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:40.989205Z","iopub.execute_input":"2025-05-27T16:17:40.99009Z","iopub.status.idle":"2025-05-27T16:17:47.198066Z","shell.execute_reply.started":"2025-05-27T16:17:40.990039Z","shell.execute_reply":"2025-05-27T16:17:47.196858Z"}},"outputs":[{"name":"stdout","text":"No GPU detected! For development, using a smaller model instead...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"709d367e146b444787d6a82247c33375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f4ab2d9c3044c9a641c4db8f45810f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a235918e23994f48880600fb75b8bfea"}},"metadata":{}},{"name":"stdout","text":"⚠️ NOTE: Using distilgpt2 instead of EleutherAI/gpt-j-6B for CPU development!\n⚠️ Remember to switch to GPU when ready for actual fine-tuning\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871f489e12d34d6485a4685cdecaad03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ca917f3a1e41f09d2f5700ec98808b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc182e24c1c4f22ac6d1af9680cd659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0470fe98d7e14d83a6dd61737aec9faf"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Configure LoRA for Parameter-Efficient Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Inspect model architecture to find correct layer names\ndef find_target_modules(model):\n    target_modules = []\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear) and module.weight.requires_grad:\n            print(f\"Found Linear layer: {name}, Shape: {module.weight.shape}\")\n            target_modules.append(name.split('.')[-1])  # Get just the final part of the name\n    \n    # Return unique module types\n    return list(set(target_modules))\n\n# Run this to see available modules\npotential_targets = find_target_modules(model)\nprint(f\"Potential target modules: {potential_targets}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.199196Z","iopub.execute_input":"2025-05-27T16:17:47.199696Z","iopub.status.idle":"2025-05-27T16:17:47.207443Z","shell.execute_reply.started":"2025-05-27T16:17:47.199661Z","shell.execute_reply":"2025-05-27T16:17:47.20599Z"}},"outputs":[{"name":"stdout","text":"Found Linear layer: lm_head, Shape: torch.Size([50257, 768])\nPotential target modules: ['lm_head']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA configuration - this reduces memory requirements significantly\npeft_config = LoraConfig(\n    r=8,                     # Rank dimension\n    lora_alpha=32,           # Alpha parameter for LoRA scaling\n    lora_dropout=0.1,        # Dropout probability for LoRA layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\"]  # Layers to apply LoRA to\n)\n\n# Apply LoRA to model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters to verify setup\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.208526Z","iopub.execute_input":"2025-05-27T16:17:47.20886Z","iopub.status.idle":"2025-05-27T16:17:47.477506Z","shell.execute_reply.started":"2025-05-27T16:17:47.208835Z","shell.execute_reply":"2025-05-27T16:17:47.475757Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/402798331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Apply LoRA to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Print trainable parameters to verify setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/mapping.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_prompt_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_prompt_learning_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     ) -> None:\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_empty_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_additional_trainable_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_new_adapter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Copy the peft_config in the injected model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36minject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_pattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" You also specified 'layers_pattern': {peft_config.layers_pattern}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;31m# Some modules did not match and some matched but were excluded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Target modules {'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again."],"ename":"ValueError","evalue":"Target modules {'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again.","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## Define Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,   # Keep small due to memory constraints\n    gradient_accumulation_steps=4,   # Accumulate gradients to simulate larger batch\n    save_steps=100,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,                       # Use mixed precision\n    report_to=\"none\",                # Disable wandb reporting\n    optim=\"adamw_torch\",\n    max_grad_norm=0.3,               # Gradient clipping\n    warmup_ratio=0.03,               # Warmup for learning rate\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.478024Z","iopub.status.idle":"2025-05-27T16:17:47.478356Z","shell.execute_reply.started":"2025-05-27T16:17:47.478199Z","shell.execute_reply":"2025-05-27T16:17:47.478212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set Up Trainer and Start Fine-Tuning","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, DataCollatorForLanguageModeling\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Define custom data formatting function\ndef formatting_func(examples):\n    texts = []\n    for i in range(len(examples[\"input\"])):\n        text = f\"<|user|>\\n{examples['input'][i]}\\n<|assistant|>\\n{examples['output'][i]}</s>\"\n        texts.append(text)\n    return texts\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.479601Z","iopub.status.idle":"2025-05-27T16:17:47.479915Z","shell.execute_reply.started":"2025-05-27T16:17:47.479778Z","shell.execute_reply":"2025-05-27T16:17:47.47979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Save the model (just the LoRA weights, not the full model)\nmodel.save_pretrained(\"./ielts_gptj_model\")\ntokenizer.save_pretrained(\"./ielts_gptj_model\")\n\n# Download the model to the local machine\nfrom IPython.display import FileLink\nFileLink(r'./ielts_gptj_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.480612Z","iopub.status.idle":"2025-05-27T16:17:47.480952Z","shell.execute_reply.started":"2025-05-27T16:17:47.480774Z","shell.execute_reply":"2025-05-27T16:17:47.48079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Test the model on an example\ntest_input = \"Rate this IELTS essay: [paste a sample essay here]\"\n\ninputs = tokenizer(test_input, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(\n    inputs.input_ids,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2\n)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:17:47.481949Z","iopub.status.idle":"2025-05-27T16:17:47.482291Z","shell.execute_reply.started":"2025-05-27T16:17:47.482153Z","shell.execute_reply":"2025-05-27T16:17:47.482166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}