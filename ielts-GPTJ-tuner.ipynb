{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/minhthonglai/ielts-gptj-tuner?scriptVersionId=242602445\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# GPT-J Fine-tuning for Essay Scoring","metadata":{}},{"cell_type":"markdown","source":"## System configurations ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when create a version using \"Save & Run All\" \n# Can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport sys\nimport os\n\n# Check Python version\nprint(f\"Python Version: `{sys.version}`\")  # Detailed version info\nprint(f\"Base Python location: `{sys.base_prefix}`\")\nprint(f\"Current Environment location: `{os.path.basename(sys.prefix)}`\", end='\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:04:49.328325Z","iopub.execute_input":"2025-05-29T18:04:49.328637Z","iopub.status.idle":"2025-05-29T18:04:49.342499Z","shell.execute_reply.started":"2025-05-29T18:04:49.328593Z","shell.execute_reply":"2025-05-29T18:04:49.341348Z"}},"outputs":[{"name":"stdout","text":"Python Version: `3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]`\nBase Python location: `/usr`\nCurrent Environment location: `usr`\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport time  # for CPU timing\n\n# Check if GPU is available and being used\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name()}\")\n    print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n\n# This code will work in both CPU and GPU environments\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n    \n# Test with a simple operation - modified to work on both CPU and GPU\nx = torch.rand(10000, 10000, device=device)\ny = torch.rand(10000, 10000, device=device)\n\nif device.type == \"cuda\":\n    # GPU timing approach\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    z = x @ y  # Matrix multiplication\n    end.record()\n    torch.cuda.synchronize()\n    print(f\"Operation time: {start.elapsed_time(end)} ms\")\nelse:\n    # CPU timing approach\n    start_time = time.time()\n    z = x @ y  # Matrix multiplication\n    end_time = time.time()\n    print(f\"Operation time: {(end_time - start_time) * 1000} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:04:49.345226Z","iopub.execute_input":"2025-05-29T18:04:49.345649Z","iopub.status.idle":"2025-05-29T18:05:13.326859Z","shell.execute_reply.started":"2025-05-29T18:04:49.34559Z","shell.execute_reply":"2025-05-29T18:05:13.325715Z"}},"outputs":[{"name":"stdout","text":"CUDA available: False\nUsing device: cpu\nOperation time: 16064.372301101685 ms\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Install required libraries","metadata":{}},{"cell_type":"code","source":"# %pip install transformers peft datasets accelerate bitsandbytes trl\n# %pip install sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:13.327837Z","iopub.execute_input":"2025-05-29T18:05:13.328247Z","iopub.status.idle":"2025-05-29T18:05:13.332999Z","shell.execute_reply.started":"2025-05-29T18:05:13.328222Z","shell.execute_reply":"2025-05-29T18:05:13.331929Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Import Libraries and Set Configuration","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport os\nfrom datasets import load_dataset\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU is available. Setting up GPU environment...\")\n    # Set maximum GPU memory usage to avoid OOM errors\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    torch.cuda.empty_cache()\nelse:\n    print(\"No GPU detected. Running in CPU-only mode.\")\n    # Ensure CUDA is disabled\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n# Enable memory efficient attention - works for both CPU and GPU\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # Work offline if model is downloaded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:13.33534Z","iopub.execute_input":"2025-05-29T18:05:13.335749Z","iopub.status.idle":"2025-05-29T18:05:47.729313Z","shell.execute_reply.started":"2025-05-29T18:05:13.335718Z","shell.execute_reply":"2025-05-29T18:05:47.728314Z"}},"outputs":[{"name":"stderr","text":"2025-05-29 18:05:28.113018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748541928.404030      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748541928.482937      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"No GPU detected. Running in CPU-only mode.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Prepare the IELTS Dataset","metadata":{}},{"cell_type":"markdown","source":"### Sample code to create a dataset for IELTS assessment\n","metadata":{}},{"cell_type":"markdown","source":"Need to prepare the dataset with this structure:\n\n```\nFormat should be:\n{\n  \"input\": \"Rate this IELTS essay: [student essay text]\",\n  \"output\": \"Task Achievement: 7.0 - [detailed feedback]\\nCoherence and Cohesion: 6.5 - [detailed feedback]\\nLexical Resource: 6.0 - [detailed feedback]\\nGrammatical Range and Accuracy: 7.0 - [detailed feedback]\\nOverall Band Score: 6.5\"\n}\n```","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\n# Example data - replace with your actual IELTS essays and scores\ndata = {\n    \"input\": [\n        \"Rate this IELTS essay: In some countries, the number of shootings increase because many people have guns at home. To what extent do you agree or disagree?\",\n        \"Rate this IELTS Task 1: The graph below shows the proportion of the population aged 65 and over between 1940 and 2040 in three different countries.\"\n    ],\n    \"output\": [\n        \"Task Achievement: 7.0 - The essay addresses all parts of the task...\\nCoherence and Cohesion: 6.5 - The essay is generally well-organized...\\nOverall Band Score: 6.5\",\n        \"Task Achievement: 6.0 - The response covers the requirements of the task...\\nCoherence and Cohesion: 6.0 - Information is arranged coherently...\\nOverall Band Score: 6.0\"\n    ]\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:47.731453Z","iopub.execute_input":"2025-05-29T18:05:47.732058Z","iopub.status.idle":"2025-05-29T18:05:47.737432Z","shell.execute_reply.started":"2025-05-29T18:05:47.73203Z","shell.execute_reply":"2025-05-29T18:05:47.736218Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = Dataset.from_dict(data)\ntrain_dataset = dataset.shuffle().select(range(int(len(dataset)*0.9)))  # 90% for training\neval_dataset = dataset.shuffle().select(range(int(len(dataset)*0.9), len(dataset)))  # 10% for evaluation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:47.738252Z","iopub.execute_input":"2025-05-29T18:05:47.738567Z","iopub.status.idle":"2025-05-29T18:05:47.825651Z","shell.execute_reply.started":"2025-05-29T18:05:47.738536Z","shell.execute_reply":"2025-05-29T18:05:47.824716Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Load GPT-J with Quantization for Memory Efficiency","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"EleutherAI/gpt-j-6B\"\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU detected! Using 4-bit quantization...\")\n    # Define quantization configuration\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    \n    # Load with quantization for GPU\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=quantization_config,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\nelse:\n    print(\"No GPU detected! For development, using a smaller model instead...\")\n    # For CPU development, consider using a much smaller model\n    # GPT-J is too large for most CPU environments\n    cpu_model_id = \"distilgpt2\"  # Only 82M parameters vs 6B\n    model = AutoModelForCausalLM.from_pretrained(\n        cpu_model_id,\n        low_cpu_mem_usage=True\n    )\n    print(f\"⚠️ NOTE: Using {cpu_model_id} instead of {model_id} for CPU development!\")\n    print(\"⚠️ Remember to switch to GPU when ready for actual fine-tuning\")\n\n# Get the tokenizer for whatever model we loaded\ntokenizer = AutoTokenizer.from_pretrained(model_id if torch.cuda.is_available() else cpu_model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Set padding token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:47.826507Z","iopub.execute_input":"2025-05-29T18:05:47.826782Z","iopub.status.idle":"2025-05-29T18:05:53.988986Z","shell.execute_reply.started":"2025-05-29T18:05:47.826758Z","shell.execute_reply":"2025-05-29T18:05:53.987735Z"}},"outputs":[{"name":"stdout","text":"No GPU detected! For development, using a smaller model instead...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d79c731f284fa5bf0a93be5987e7d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa7cd63b510495eb11d2ac8d648f279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6f25c0d83746a9b6dd8c21717f063e"}},"metadata":{}},{"name":"stdout","text":"⚠️ NOTE: Using distilgpt2 instead of EleutherAI/gpt-j-6B for CPU development!\n⚠️ Remember to switch to GPU when ready for actual fine-tuning\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4121dfb4d63b4836931e7ba4f4f54f00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d2f48051584b46b151210c452b9c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f22c863895845d4ac80b35e31904bad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e811f097d84a838f224737d76d4766"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Configure LoRA for Parameter-Efficient Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Inspect model architecture to find correct layer names\ndef find_target_modules(model):\n    target_modules = []\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear) and module.weight.requires_grad:\n            print(f\"Found Linear layer: {name}, Shape: {module.weight.shape}\")\n            target_modules.append(name.split('.')[-1])  # Get just the final part of the name\n    \n    # Return unique module types\n    return list(set(target_modules))\n\n# Run this to see available modules\npotential_targets = find_target_modules(model)\nprint(f\"Potential target modules: {potential_targets}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:53.990583Z","iopub.execute_input":"2025-05-29T18:05:53.99098Z","iopub.status.idle":"2025-05-29T18:05:54.000346Z","shell.execute_reply.started":"2025-05-29T18:05:53.990948Z","shell.execute_reply":"2025-05-29T18:05:53.99867Z"}},"outputs":[{"name":"stdout","text":"Found Linear layer: lm_head, Shape: torch.Size([50257, 768])\nPotential target modules: ['lm_head']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# For DistilGPT2/GPT-2 style models:\npeft_config = LoraConfig(\n    r=8,                     # Rank dimension\n    lora_alpha=32,           # Alpha parameter for LoRA scaling\n    lora_dropout=0.1,        # Dropout probability for LoRA layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"c_attn\", \"c_proj\"],  # Standard GPT-2 attention layers\n)\n\n# Print configuration\nprint(f\"Using target modules for GPT-2 style architecture: ['c_attn', 'c_proj']\")\n\n# Apply LoRA to model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters to verify setup\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.001703Z","iopub.execute_input":"2025-05-29T18:05:54.002104Z","iopub.status.idle":"2025-05-29T18:05:54.147262Z","shell.execute_reply.started":"2025-05-29T18:05:54.002066Z","shell.execute_reply":"2025-05-29T18:05:54.145894Z"}},"outputs":[{"name":"stdout","text":"Using target modules for GPT-2 style architecture: ['c_attn', 'c_proj']\ntrainable params: 405,504 || all params: 82,318,080 || trainable%: 0.4926\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Define Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,   # Keep small due to memory constraints\n    gradient_accumulation_steps=4,   # Accumulate gradients to simulate larger batch\n    save_steps=100,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True if torch.cuda.is_available() else False, # Use mixed precision\n    report_to=\"none\",                # Disable wandb reporting\n    optim=\"adamw_torch\",\n    max_grad_norm=0.3,               # Gradient clipping\n    warmup_ratio=0.03,               # Warmup for learning rate\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.148228Z","iopub.execute_input":"2025-05-29T18:05:54.148492Z","iopub.status.idle":"2025-05-29T18:05:54.215012Z","shell.execute_reply.started":"2025-05-29T18:05:54.148469Z","shell.execute_reply":"2025-05-29T18:05:54.214008Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class IELTSDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def __call__(self, examples):\n        # Extract input and output\n        inputs = [example[\"input\"] for example in examples]\n        outputs = [example[\"output\"] for example in examples]\n        \n        # Format the text (input followed by output)\n        texts = [f\"<|user|>\\n{inp}\\n<|assistant|>\\n{out}</s>\" for inp, out in zip(inputs, outputs)]\n        \n        # Tokenize\n        batch = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n        \n        # For causal language modeling, use the input_ids as labels too\n        batch[\"labels\"] = batch[\"input_ids\"].clone()\n        \n        return batch\n\n# Create the custom data collator\ndata_collator = IELTSDataCollator(tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.216153Z","iopub.execute_input":"2025-05-29T18:05:54.216498Z","iopub.status.idle":"2025-05-29T18:05:54.224402Z","shell.execute_reply.started":"2025-05-29T18:05:54.21647Z","shell.execute_reply":"2025-05-29T18:05:54.223231Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Format the text\n    texts = [\n        f\"<|user|>\\n{examples['input'][i]}\\n<|assistant|>\\n{examples['output'][i]}</s>\"\n        for i in range(len(examples['input']))\n    ]\n    \n    # Tokenize\n    tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n    \n    # For language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    \n    return tokenized\n\n# Process the dataset\nprocessed_dataset = train_dataset.map(\n    preprocess_function, \n    batched=True,\n    remove_columns=train_dataset.column_names  # Remove the original columns\n)\n\n# Define the Trainer with processed dataset\ntrainer = Trainer(\n    model=model,\n    args=training_args,  # No need for remove_unused_columns=False here\n    train_dataset=processed_dataset,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.22537Z","iopub.execute_input":"2025-05-29T18:05:54.225651Z","iopub.status.idle":"2025-05-29T18:05:54.604774Z","shell.execute_reply.started":"2025-05-29T18:05:54.225604Z","shell.execute_reply":"2025-05-29T18:05:54.603339Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af59bdb30ab149bd8dc18059e654ded5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/287235684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Define the Trainer with processed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# No need for remove_unused_columns=False here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"],"ename":"NameError","evalue":"name 'Trainer' is not defined","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## Set Up Trainer and Start Fine-Tuning","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, DataCollatorForLanguageModeling\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Define custom data formatting function\ndef formatting_func(examples):\n    texts = []\n    for i in range(len(examples[\"input\"])):\n        text = f\"<|user|>\\n{examples['input'][i]}\\n<|assistant|>\\n{examples['output'][i]}</s>\"\n        texts.append(text)\n    return texts\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.605489Z","iopub.status.idle":"2025-05-29T18:05:54.605896Z","shell.execute_reply.started":"2025-05-29T18:05:54.605705Z","shell.execute_reply":"2025-05-29T18:05:54.605724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Save the model (just the LoRA weights, not the full model)\nmodel.save_pretrained(\"./ielts_gptj_model\")\ntokenizer.save_pretrained(\"./ielts_gptj_model\")\n\n# Download the model to the local machine\nfrom IPython.display import FileLink\nFileLink(r'./ielts_gptj_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.607508Z","iopub.status.idle":"2025-05-29T18:05:54.607924Z","shell.execute_reply.started":"2025-05-29T18:05:54.60772Z","shell.execute_reply":"2025-05-29T18:05:54.607739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Test the model on an example\ntest_input = \"Rate this IELTS essay: [paste a sample essay here]\"\n\ninputs = tokenizer(test_input, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(\n    inputs.input_ids,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2\n)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:05:54.609468Z","iopub.status.idle":"2025-05-29T18:05:54.609883Z","shell.execute_reply.started":"2025-05-29T18:05:54.60964Z","shell.execute_reply":"2025-05-29T18:05:54.609658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}